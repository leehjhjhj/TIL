# 스파크 문법 뜯어보기

## SparkSession

```python
spark = SparkSession.builder.appName("PopularMovies").getOrCreate()
```

- `SparkSession`
    - 파크 2.0에서의 작업을 위한 진입점. **SparkContext, SQLContext, HiveContext** 등 이전 버전의 여러 컨텍스트를 통합
    - 이 객체를 통해서 DataFrame과 Dataset API를 사용하여 데이터 처리 가능
    - `appName()`: 애플리케이션 이름을 설정
    - `config()`: 설정이 있다면 추가
    - `getOrCreate()`: SparkSession이 이미 존재하면 반환, 그렇지 않으면 생성한다.
    - .`sql()`: 메서드를 사용해서 SQL 쿼리 수행 가능
    - `UDF(User Defined Functions) 등록`: 사용자 정의 함수를 SQL 쿼리에서 사용 가능
    - `stop()`:  세션을 종료. 리소스를 정리

## DataFrame 만들기

### 강의에서 생성한 방법

```python
lines = spark.sparkContext.textFile("hdfs:///user/maria_dev/ml-100k/u.data")
# Convert it to a RDD of Row objects with (movieID, rating)
movies = lines.map(parseInput)
# Convert that to a DataFrame
movieDataset = spark.createDataFrame(movies)
```

1. SparkContext를 이용해 텍스트 파일을 읽어 RDD 생성
2. Row 객체의 RDD로 변환
3. 이것을 DataFrame으로 변환

### 다른 방법

```python
# CSV 파일로부터 DataFrame 생성
df_csv = spark.read.csv("path/to/csvfile.csv", inferSchema=True, header=True)

# JSON 파일로부터 DataFrame 생성
df_json = spark.read.json("path/to/jsonfile.json")

# Parquet 파일로부터 DataFrame 생성
df_parquet = spark.read.parquet("path/to/parquetfile.parquet")
```

- 이렇게 다양한 외부 데이터 소스로부터 직접 Dataframe을 생성하는 기능도 있다.

```python
import pandas as pd

# Pandas DataFrame 생성
pandas_df = pd.DataFrame({"A": [1, 2, 3], "B": [4, 5, 6]})

# Spark DataFrame으로 변환
spark_df = spark.createDataFrame(pandas_df)
```

- 놀랍게도 판다스의 데이터 프레임과도 연동이 가능

```python
data = [("James", "Sales", 3000),
        ("Michael", "Sales", 4600),
        ("Robert", "Sales", 4100)]
columns = ["Name", "Department", "Salary"]
df = spark.createDataFrame(data, schema=columns)

# lit 함수를 사용하여 모든 행에 'Employee'라는 값을 가지는 새 컬럼 'Status' 추가
df = df.withColumn("Status", lit("Employee"))

df.show()
```

- 아니면 저렇게 리스트 안의 튜플, 그리고 column의 정보가 들어있는 리스트로 DataFrame을 만들 수 있다.
- `lit` 을 통해서 스파크 데이터 타입으로 바꿔줄 수 있다.
    - 해당 예시는 `Status` 라는 컬럼의 값으로 `Employee` 를 추가하는 예시이다.
    - 리터럴(literal) 값을 생성하여 데이터 프레임의 컬럼에 추가하거나 연산에 사용할 때 유용
    
    ```python
    from pyspark.sql.functions import when
    
    # 조건에 따라 새로운 컬럼 추가
    df = df.withColumn("Status", when(df["ID"] == 1, lit("One")).otherwise(lit("Other")))
    
    df.show()
    
    # 출력 예시
    +-----+---+---------+------+
    | Name| ID|NewColumn|Status|
    +-----+---+---------+------+
    |Alice|  1|lit_value|   One|
    |  Bob|  2|lit_value| Other|
    +-----+---+---------+------+
    
    # 새로운 컬럼 추가: ID 값에 10을 더한 값
    df = df.withColumn("ID_plus_10", df["ID"] + lit(10))
    
    df.show()
    
    # 출력 예시
    +-----+---+---------+------+----------+
    | Name| ID|NewColumn|Status|ID_plus_10|
    +-----+---+---------+------+----------+
    |Alice|  1|lit_value|   One|        11|
    |  Bob|  2|lit_value| Other|        12|
    +-----+---+---------+------+----------+
    ```
    

## 기본 문법

```python
# DataFrame 스키마 출력
df.printSchema()

# DataFrame의 첫 5줄 출력
df.show(5)

# 특정 컬럼 선택
df.select("name", "age").show()

# 특정 컬럼을 제외하고 불러올 때
df.drop("name").show()

# 조건 필터링
df.filter(df['age'] > 21).show()

# 그룹화 및 집계
df.groupBy("age").count().show()

# 정렬
df.orderBy(asc("state"),desc("gender")).show()
```

- `select` 는 SQL의 select와 비슷하다.
    
    ```python
    df.select(count("name"), countDistinct("name")).show()
    ```
    
- `filter`와 `where`는 같은 결과를 낸다.
    
    ```python
    df.filter((df.state == "OH") & (df.gender == "M")).show()
    ```
    
    - 여러 조건 절을 파이썬 문법을 사용해 표현 가능
    
    ```python
    # 10, 20, 30 중 하나에 해당하는 행 필터링
    df.filter(df.Age.isin(10, 20, 30)).show()
    
    # 10, 20, 30 중 하나에 해당하지 않는 행 필터링 (NOT IN)
    df.filter(~df.Age.isin(10, 20, 30)).show()
    ```
    
    - not in은 앞에 `~` 를 추가한다.
- join도 가능하고, 어떤 join인지도 선택 가능하다.
    
    ```python
    inner_join_df = df1.join(df2, on="Name", how="inner")
    inner_join_df.show()
    ```
    
    - how에는 다들 아는 조인 방법들이 들어감
        - inner
        - left
        - right
        - outer
        - `cross_join_df = df1.crossJoin(df2)`